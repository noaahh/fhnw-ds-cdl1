{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T14:59:48.502174Z",
     "start_time": "2024-06-01T14:59:46.910036Z"
    }
   },
   "cell_type": "code",
   "source": "import lightning as L\n",
   "id": "a96227b85a1b70b3",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T14:59:48.515325Z",
     "start_time": "2024-06-01T14:59:48.505403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class sLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.0):\n",
    "        super(sLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lstms = nn.ModuleList([nn.LSTMCell(input_size if i == 0 else hidden_size, hidden_size) for i in range(num_layers)])\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "        self.exp_forget_gates = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(num_layers)])\n",
    "        self.exp_input_gates = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(num_layers)])\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for lstm in self.lstms:\n",
    "            nn.init.xavier_uniform_(lstm.weight_ih)\n",
    "            nn.init.xavier_uniform_(lstm.weight_hh)\n",
    "            nn.init.zeros_(lstm.bias_ih)\n",
    "            nn.init.zeros_(lstm.bias_hh)\n",
    "\n",
    "        for gate in self.exp_forget_gates + self.exp_input_gates:\n",
    "            nn.init.xavier_uniform_(gate.weight)\n",
    "            nn.init.zeros_(gate.bias)\n",
    "\n",
    "    def forward(self, input_seq, hidden_state=None):\n",
    "        print(f\"input_seq: {type(input_seq)}\")\n",
    "        batch_size = input_seq.size(0)\n",
    "        seq_length = input_seq.size(1)\n",
    "\n",
    "        if hidden_state is None:\n",
    "            hidden_state = self.init_hidden(batch_size)\n",
    "\n",
    "        output_seq = []\n",
    "        for t in range(seq_length):\n",
    "            x = input_seq[:, t, :]\n",
    "            new_hidden_state = []\n",
    "            for i, (lstm, f_gate, i_gate) in enumerate(zip(self.lstms, self.exp_forget_gates, self.exp_input_gates)):\n",
    "                if hidden_state[i][0] is None:\n",
    "                    h, c = lstm(x)\n",
    "                else:\n",
    "                    h, c = lstm(x, (hidden_state[i][0], hidden_state[i][1]))\n",
    "\n",
    "                f = torch.exp(f_gate(h))\n",
    "                i_g = torch.exp(i_gate(h))\n",
    "                c = f * c + i_g * lstm.weight_hh.new_zeros(batch_size, self.hidden_size)\n",
    "                new_hidden_state.append((h, c))\n",
    "\n",
    "                if i < self.num_layers - 1:\n",
    "                    x = self.dropout_layer(h)\n",
    "                else:\n",
    "                    x = h\n",
    "            hidden_state = new_hidden_state\n",
    "            output_seq.append(x)\n",
    "\n",
    "        output_seq = torch.stack(output_seq, dim=1)\n",
    "        return output_seq, hidden_state\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden_state = []\n",
    "        for lstm in self.lstms:\n",
    "            h = torch.zeros(batch_size, self.hidden_size, device=lstm.weight_ih.device)\n",
    "            c = torch.zeros(batch_size, self.hidden_size, device=lstm.weight_ih.device)\n",
    "            hidden_state.append((h, c))\n",
    "        return hidden_state"
   ],
   "id": "c497bedaf67f9808",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T14:59:48.531531Z",
     "start_time": "2024-06-01T14:59:48.516516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.models.xlstm.m_lstm import mLSTM\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Accuracy, F1Score\n",
    "\n",
    "\n",
    "class xLSTMBlock(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.0, bidirectional=False, lstm_type=\"slstm\"):\n",
    "        super(xLSTMBlock, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm_type = lstm_type\n",
    "\n",
    "        if lstm_type == \"slstm\":\n",
    "            self.lstm = sLSTM(input_size, hidden_size, num_layers, dropout)\n",
    "        elif lstm_type == \"mlstm\":\n",
    "            print(\"Warning: mLSTM is not working yet.\")\n",
    "            self.lstm = mLSTM(input_size, hidden_size, num_layers, dropout)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid LSTM type: {lstm_type}\")\n",
    "\n",
    "        self.norm = nn.LayerNorm(input_size)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "        if bidirectional:\n",
    "            self.proj = nn.Linear(2 * hidden_size, input_size)\n",
    "        else:\n",
    "            self.proj = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "        # print shapes\n",
    "        print(f\"input_size: {input_size}\")\n",
    "        print(f\"hidden_size: {hidden_size}\")\n",
    "        print(f\"num_layers: {num_layers}\")\n",
    "        print(f\"dropout: {dropout}\")\n",
    "        print(f\"proj: {self.proj}\")\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.proj.weight)\n",
    "        nn.init.zeros_(self.proj.bias)\n",
    "\n",
    "    def forward(self, input_seq, hidden_state=None):\n",
    "        lstm_output, hidden_state = self.lstm(input_seq, hidden_state)\n",
    "        if self.lstm_type == \"slstm\":\n",
    "            hidden_state = [[hidden_state[i][0].detach(), hidden_state[i][1].detach()] for i in range(len(hidden_state))]\n",
    "\n",
    "        if self.bidirectional:\n",
    "            lstm_output = torch.cat((lstm_output[:, :, :self.hidden_size], lstm_output[:, :, self.hidden_size:]), dim=-1)\n",
    "\n",
    "        output = self.activation(self.proj(lstm_output))\n",
    "        output = self.norm(output + input_seq)\n",
    "        output = self.dropout_layer(output)\n",
    "\n",
    "        return output, hidden_state\n",
    "\n",
    "class xLSTM(L.LightningModule):\n",
    "    def __init__(self, optimizer, scheduler, input_size, hidden_size, output_size, num_layers, num_blocks,\n",
    "                 dropout=0.0, bidirectional=False, lstm_type=\"slstm\"):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.accuracy = Accuracy(task='multiclass', num_classes=output_size)\n",
    "        self.f1_score = F1Score(num_classes=output_size, average='weighted', task='multiclass')\n",
    "        self.num_blocks = num_blocks\n",
    "        self.lstm_type = lstm_type\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            xLSTMBlock(input_size, hidden_size, num_layers,\n",
    "                       dropout, bidirectional, lstm_type)\n",
    "            for i in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq, hidden_states=None):\n",
    "        if hidden_states is None:\n",
    "            hidden_states = [None] * self.num_blocks\n",
    "\n",
    "        output_seq = input_seq\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            output_seq, hidden_state = block(output_seq, hidden_states[i])\n",
    "            if self.lstm_type == \"slstm\":\n",
    "                hidden_states[i] = [[hidden_state[j][0].detach(), hidden_state[j][1].detach()] for j in range(len(hidden_state))]\n",
    "            else:\n",
    "                hidden_states[i] = hidden_state\n",
    "\n",
    "        output_seq = output_seq[:, -1, :]\n",
    "        output_seq = self.output_layer(output_seq)\n",
    "        print(f\"output_seq: {output_seq.shape}\")\n",
    "        return output_seq\n",
    "\n",
    "    def _shared_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        y = torch.argmax(y, dim=1)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = self.accuracy(preds, y)\n",
    "        f1 = self.f1_score(preds, y)\n",
    "        return loss, acc, f1\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc, f1 = self._shared_step(batch, batch_idx)\n",
    "        self.log_dict({\"train_loss\": loss, \"train_acc\": acc, \"train_f1\": f1}, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc, f1 = self._shared_step(batch, batch_idx)\n",
    "        self.log_dict({\"val_loss\": loss, \"val_acc\": acc, \"val_f1\": f1}, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.hparams.optimizer(params=self.trainer.model.parameters())\n",
    "        scheduler = self.hparams.scheduler(optimizer, T_max=10)\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            }\n",
    "        }"
   ],
   "id": "65d54d779385e3e3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T14:59:51.120120Z",
     "start_time": "2024-06-01T14:59:48.533060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.data.dataset import SensorDataModule\n",
    "\n",
    "dataset = SensorDataModule(32, \"../data/partitions\", k_folds=0)\n",
    "dataset.setup()\n",
    "\n",
    "train_dataloader, val_dataloader = dataset.train_dataloader(), dataset.val_dataloader()\n",
    "trainer = L.Trainer(max_epochs=5,\n",
    "                     accelerator='cpu',\n",
    "                     log_every_n_steps=10)\n"
   ],
   "id": "63a1799756621a77",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/dmnk/PycharmProjects/cdl1-sensor-based/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/dmnk/PycharmProjects/cdl1-sensor-based/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T15:00:25.648896Z",
     "start_time": "2024-06-01T14:59:51.120868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "batch = next(iter(train_dataloader))\n"
   ],
   "id": "b7bdb22636a2eb46",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "batch[0].shape",
   "id": "9dcdfd1176c3fc89",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 251, 16])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T15:00:26.070811Z",
     "start_time": "2024-06-01T15:00:25.673042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = xLSTM(input_size=16,\n",
    "                         hidden_size=24,\n",
    "                         output_size=dataset.num_classes,\n",
    "                         num_layers=3,\n",
    "                         dropout=0.2,\n",
    "                         num_blocks=2, optimizer=torch.optim.Adam, scheduler=torch.optim.lr_scheduler.CosineAnnealingLR)\n",
    "\n",
    "model.training_step(batch, 0)"
   ],
   "id": "6243825ac135fdba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size: 16\n",
      "hidden_size: 24\n",
      "num_layers: 3\n",
      "dropout: 0.2\n",
      "proj: Linear(in_features=24, out_features=16, bias=True)\n",
      "input_size: 16\n",
      "hidden_size: 24\n",
      "num_layers: 3\n",
      "dropout: 0.2\n",
      "proj: Linear(in_features=24, out_features=16, bias=True)\n",
      "input_seq: <class 'torch.Tensor'>\n",
      "input_seq: <class 'torch.Tensor'>\n",
      "output_seq: torch.Size([32, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmnk/PycharmProjects/cdl1-sensor-based/.venv/lib/python3.11/site-packages/lightning/pytorch/core/module.py:436: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.9942, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T15:00:26.073600Z",
     "start_time": "2024-06-01T15:00:26.071866Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "db75c1eeef06bc81",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T15:00:26.075831Z",
     "start_time": "2024-06-01T15:00:26.074483Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e6d6a1f707b13f91",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T15:00:26.080763Z",
     "start_time": "2024-06-01T15:00:26.079263Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "da8a0845aab1dd40",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T15:00:26.083044Z",
     "start_time": "2024-06-01T15:00:26.081661Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e7479cbd65008fe0",
   "outputs": [],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
