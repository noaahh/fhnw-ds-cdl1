{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Accuracy, F1Score"
   ],
   "id": "a96227b85a1b70b3",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class BidirectionalLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True,\n",
    "                            dropout=dropout if num_layers > 1 else 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Before LSTM: {x.shape}\")\n",
    "        x, _ = self.lstm(x)\n",
    "        print(f\"After LSTM: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout, num_blocks):\n",
    "        super().__init__()\n",
    "\n",
    "        self.first_linear = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "        output_dim = hidden_dim * 2\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_blocks):\n",
    "            # input_dim = input_dim * (i + 1)\n",
    "\n",
    "            print(f\"Block {i}, input_dim: {input_dim}, output_dim: {output_dim}\")\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    BidirectionalLayer(input_dim, hidden_dim, num_layers, dropout),\n",
    "                    nn.Linear(output_dim, input_dim),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm1d(input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_linear(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        residual = None\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # residual = x if i == 0 else self.layers[i - 1][1](x)\n",
    "            layer_output = layer(x)\n",
    "\n",
    "            print(\n",
    "                f\"Layer {i} input shape: {x.shape}, residual shape: {residual.shape if residual is not None else None}, layer_output shape: {layer_output.shape}\")\n",
    "\n",
    "            if i != 0:\n",
    "                x = layer_output + residual\n",
    "            else:\n",
    "                x = layer_output\n",
    "\n",
    "            residual = x\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.batch_norm(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepResBidirLSTM(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout, num_blocks, output_dim):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.residual_layer = ResidualLayer(input_dim, hidden_dim, num_layers, dropout, num_blocks)\n",
    "        self.final_fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "        self.accuracy = Accuracy(task='multiclass', num_classes=output_dim)\n",
    "        self.f1_score = F1Score(num_classes=output_dim, average='weighted', task='multiclass')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.residual_layer(x)\n",
    "        x = x[:, -1, :]  # taking last timestep's output\n",
    "        x = self.final_fc(x)\n",
    "        return x\n",
    "\n",
    "    def _shared_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        logits = self(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        y = torch.argmax(y, dim=1)\n",
    "        loss = F.cross_entropy(logits, y.float())\n",
    "        acc = self.accuracy(preds, y)\n",
    "        f1 = self.f1_score(preds, y)\n",
    "        print(f\"preds: {preds}, y: {y}, acc: {acc}, f1: {f1}\")\n",
    "\n",
    "        return loss, acc, f1\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc, f1 = self._shared_step(batch, batch_idx)\n",
    "        self.log_dict({\"train_loss\": loss, \"train_acc\": acc, \"train_f1\": f1}, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc, f1 = self._shared_step(batch, batch_idx)\n",
    "        self.log_dict({\"val_loss\": loss, \"val_acc\": acc, \"val_f1\": f1}, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n"
   ],
   "id": "65d54d779385e3e3",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.data.sensor_datamodule import SensorDataModule\n",
    "from src.utils import get_partition_paths\n",
    "\n",
    "dataset = SensorDataModule(32, partition_paths=get_partition_paths(\"./data/partitions\", k_folds=5))\n",
    "dataset.setup()\n",
    "\n",
    "for key, fold in dataset.data_dict.items():\n",
    "    train_dataloader, val_dataloader = fold['train'], fold['validate']\n",
    "    trainer = pl.Trainer(max_epochs=5,\n",
    "                         devices=1,\n",
    "                         accelerator='mps',\n",
    "\n",
    "                         gradient_clip_val=15,\n",
    "                         gradient_clip_algorithm='norm',\n",
    "\n",
    "                         log_every_n_steps=10)\n",
    "\n",
    "    model = DeepResBidirLSTM(input_dim=16,\n",
    "                             hidden_dim=24,\n",
    "                             output_dim=dataset.num_classes,\n",
    "                             num_layers=3,\n",
    "                             dropout=0.2,\n",
    "                             num_blocks=2)\n",
    "\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "    break  # only train one fold for now"
   ],
   "id": "20a6faf6f644c413",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
