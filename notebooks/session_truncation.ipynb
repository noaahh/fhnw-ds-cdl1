{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "from omegaconf import OmegaConf\n",
    "from src.data_pipeline import prepare_time_series_segments"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Session Truncation\n",
    "\n",
    "In this notebook, we will explore the effect of truncating sessions to a fixed length. Our dataset includes five different activities, each with unique physical demands, resulting in varying session lengths. For instance, running typically results in longer sessions compared to sitting due to its higher physical intensity.\n",
    "\n",
    "This variation in session lengths poses a challenge when splitting the data into train and test sets, especially when done by session. For example, consider a 20-minute running session. If we divide this session into 5-second segments, we get 240 segments. When we split the data by session, all 240 segments from this running session will end up entirely in either the train or test set, leading to an imbalance.\n",
    "\n",
    "To address this issue, we can truncate sessions to a fixed length, ensuring a more balanced and representative distribution of segments across both train and test sets. This method allows us to maintain the diversity of activities within each set and improve the robustness of our model."
   ],
   "id": "41423b9952659f41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Data\n",
    "\n",
    "We load the raw data from the cache and prepare the time series segments using the `prepare_time_series_segments` function. We then explore the distribution of session lengths before and after truncation to evaluate the effectiveness of the truncation process."
   ],
   "id": "178281cfabd89ac1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = pd.read_parquet(\"../data/cache/raw_data_db_cache.parquet\")\n",
    "data.shape"
   ],
   "id": "ef6644280ea4a248",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After the data is loaded we will run a part of the `data_pipeline.py` script to prepare the time series segments. We will then explore the distribution of session lengths before and after truncation.",
   "id": "a94bf2b13fd1a5c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "crop_start_s = 5\n",
    "crop_end_s = 5\n",
    "resample_rate_hz = 50\n",
    "segment_size_s = 5\n",
    "overlap_s = 0\n",
    "\n",
    "cfg = {\n",
    "    \"preprocessing\": {\n",
    "        \"crop\": {\n",
    "            \"start_seconds\": crop_start_s,\n",
    "            \"end_seconds\": crop_end_s,\n",
    "        },\n",
    "        \"resample_rate_hz\": resample_rate_hz,\n",
    "        \"segment_size_seconds\": segment_size_s,\n",
    "        \"overlap_seconds\": overlap_s,\n",
    "        \"smoothing\": {\n",
    "            \"type\": \"null\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "cfg = OmegaConf.create(cfg)\n",
    "\n",
    "segments_df = prepare_time_series_segments(data,\n",
    "                                           cfg)\n",
    "\n",
    "import logging\n",
    "logging.disable()\n",
    "\n",
    "segments_df.shape"
   ],
   "id": "6fb59ea59194a809",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The preparation of the time series segments leaves us with a total of 1092101 segments. We will now explore the distribution of session lengths before and after truncation.",
   "id": "780b371d3437bc10"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Session Length Distribution",
   "id": "e3e4a0cc1a0eef91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_session_lengths(df):\n",
    "    return (df.groupby(\"session_id\")\n",
    "            .agg({\"session_id\": \"count\"})\n",
    "            .rename(columns={\"session_id\": \"count\"}) / resample_rate_hz)\n",
    "\n",
    "\n",
    "get_session_lengths(segments_df).plot.hist()\n",
    "get_session_lengths(segments_df).describe()"
   ],
   "id": "74f0a7c562bbf30f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The distribution of session lengths before truncation shows a wide range of session lengths, with a mean of 186 seconds and a standard deviation of 277 seconds. We will now truncate the sessions to a fixed length of 5 minutes and explore the distribution of session lengths after truncation to verify the effectiveness of the truncation. We can see that there are some sessions that are very long, and some that are relatively short.\n",
    "\n",
    "We can calculate the number of segments that fit into a 180 seconds session to determine the maximum number of segments per session after truncation."
   ],
   "id": "279804acd90c9c1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "file_length_limit_s = 180\n",
    "max_count_segments = math.floor(file_length_limit_s / segment_size_s)\n",
    "max_count_segments"
   ],
   "id": "60b52e08338139ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Truncate Sessions",
   "id": "42892c7cbce274d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For a maximum of 5 minutes per session, we can fit a maximum of 36 segments per session. We will now truncate the sessions to this fixed length and explore the distribution of session lengths after truncation.\n",
    "\n",
    "1. **DataFrame Grouping:**\n",
    "   ```python\n",
    "   segments_truncated_df = segments_df.groupby('session_id').apply(\n",
    "   ```\n",
    "   This part of the code groups the `segments_df` DataFrame by the `session_id` column. Each group will contain all segments belonging to a particular session.\n",
    "\n",
    "2. **Lambda Function:**\n",
    "   ```python\n",
    "   lambda x: x[x['segment_id'].isin(\n",
    "   ```\n",
    "   For each group (i.e., each session), a lambda function is applied. `x` represents each group (session) DataFrame. The lambda function is used to filter the segments within each session.\n",
    "\n",
    "3. **Drop Duplicates and Sample Segments:**\n",
    "   ```python\n",
    "   x['segment_id'].drop_duplicates().sample(n=min(len(x['segment_id'].drop_duplicates()), max_count_segments))\n",
    "   ```\n",
    "   Within each session, duplicate `segment_id` values are dropped using `drop_duplicates()`, ensuring each segment is unique. Then, a sample of these unique segments is taken.\n",
    "\n",
    "   - `x['segment_id'].drop_duplicates()`: Drops duplicate segment IDs within the session.\n",
    "   - `.sample(n=...)`: Samples a number of segments. The number of segments to sample is determined by the `min` function:\n",
    "     - `len(x['segment_id'].drop_duplicates())`: Total number of unique segments in the session.\n",
    "     - `max_count_segments`: The maximum number of segments allowed per session (60 in this case).\n",
    "      - Through random sampling, we ensure that the truncation process does not bias the selection of segments within each session.\n",
    "\n",
    "   The `min` function ensures that if a session has fewer than 60 segments, all of them are taken. If a session has more than 60 segments, only 60 are randomly sampled.\n",
    "\n",
    "4. **Filter the Segments:**\n",
    "   ```python\n",
    "   x[x['segment_id'].isin(...)]\n",
    "   ```\n",
    "   The `isin` method is used to filter the original segments DataFrame (`x`) to include only the sampled segment IDs.\n",
    "\n",
    "5. **Reset Index:**\n",
    "   ```python\n",
    "   ).reset_index(drop=True)\n",
    "   ```\n",
    "   After applying the lambda function and filtering the segments, the index is reset to avoid retaining the original index from the grouped DataFrame.\n",
    "\n",
    "The final result is a new DataFrame `segments_truncated_df` where each session contains a maximum of 60 unique segments, or all segments if the session originally had fewer than 60 segments."
   ],
   "id": "18d616866bb94e05"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "segments_truncated_df = segments_df.groupby('session_id').apply(\n",
    "    lambda x: x[x['segment_id'].isin(\n",
    "        x['segment_id'].drop_duplicates().sample(n=min(len(x['segment_id'].drop_duplicates()), max_count_segments))\n",
    "    )]\n",
    ").reset_index(drop=True)"
   ],
   "id": "bf039c238d2012d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "get_session_lengths(segments_truncated_df).plot.hist()\n",
    "get_session_lengths(segments_truncated_df).describe() "
   ],
   "id": "4561f81941f702fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After truncating the sessions to a fixed length of 5 minutes, the distribution of session lengths shows a narrower range of session lengths, with a mean of 95 seconds and a standard deviation of 72 seconds. The truncation process has effectively reduced the variation in session lengths, ensuring a more balanced distribution of segments across sessions. It is visible that the sessions lengths from the upper gather on the upper end of the distribution have been truncated to the maximum length of 180 secons.",
   "id": "344d05c94256fe13"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
