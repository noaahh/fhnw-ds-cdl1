{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as signal\n",
    "\n",
    "np.random.seed(1337)"
   ],
   "id": "980eba435c8f16eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preprocessing\n",
    "\n",
    "This notebooks explore the preprocessing steps for time series data ultimately implemented as part of the `data_pipeline.py` script. The preprocessing steps include:\n",
    "\n",
    "1. **Time Series Preprocessing**: This includes converting the time column to a datetime object and setting it as the index.\n",
    "2. **Crop**: This involves cropping the data to remove the first and last few seconds of the recording.\n",
    "3. **Resample**: This involves resampling the data to a lower frequency.\n",
    "4. **Segmentation**: This involves segmenting the data into smaller chunks.\n",
    "5. **Feature Extraction**: This involves extracting features from the data.\n",
    "6. **Smoothing**: This involves applying filters to smooth the data.\n",
    "\n",
    "Let's start by loading the data and visualizing it.\n",
    "\n",
    "## Load Data\n",
    "\n",
    "For the purpose of this notebook, we will load a random file from the dataset."
   ],
   "id": "13fc8569c8a62822"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "data = pd.read_parquet(\"../data/cache/raw_data_db_cache.parquet\")\n",
    "data = data[data['file_hash'] == data['file_hash'].sample(1).values[0]] # select a random file\n",
    "\n",
    "data.drop(columns=['recording_time', 'result', 'table'], inplace=True)\n",
    "data.columns = data.columns.str.replace('_time', 'time')\n",
    "data = data.rename(columns={'accelerometer_x': 'x', 'accelerometer_y': 'y', 'accelerometer_z': 'z'})\n",
    "\n",
    "data = data.sort_values('time')\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "sensor_data = data.copy()\n",
    "data.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Afterwards we can define a function to visualize the data.",
   "id": "da7ab0e68a621ebd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_data(data, title, first_n_seconds=None):\n",
    "    if first_n_seconds is not None:\n",
    "        data = data[data.index <= data.index[0] + pd.Timedelta(seconds=first_n_seconds)]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(data.index, data['x'], label='X-axis')\n",
    "    plt.plot(data.index, data['y'], label='Y-axis')\n",
    "    plt.plot(data.index, data['z'], label='Z-axis')\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Acceleration')\n",
    "    plt.show()"
   ],
   "id": "cc045370b39f6e2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Time Series Preprocessing\n",
    "\n",
    "The first step is to preprocess the time series data by converting the time column to a datetime object and setting it as the index. Afterwards we ensure the data is sorted by time."
   ],
   "id": "4a63acd028091e3"
  },
  {
   "cell_type": "code",
   "source": [
    "sensor_data['time'] = pd.to_datetime(sensor_data['time'], unit='ns')\n",
    "sensor_data = sensor_data.set_index('time', drop=True)\n",
    "sensor_data = sensor_data.sort_index()\n",
    "visualize_data(sensor_data.copy(), 'Preprocessed Data', first_n_seconds=5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ddee27b7941bfa5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can already see on the visualised data, the data is quite noisy in the first few seconds. This comes from the measurement device being moved into position as part of the measurement concept. This is also the case for the last few seconds of the recording. We can remove this noise by cropping the data.",
   "id": "d0e46ca0efe8a8ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Crop\n",
    "\n",
    "The next step is to crop the data to remove the first and last few seconds of the recording. This can be done by specifying the start and end time to crop the data."
   ],
   "id": "41bc91974bece3ba"
  },
  {
   "cell_type": "code",
   "source": [
    "start_crop = pd.Timedelta(seconds=5)\n",
    "end_crop = pd.Timedelta(seconds=5)\n",
    "\n",
    "cropped_data = sensor_data[sensor_data.index.min() + start_crop:sensor_data.index.max() - end_crop]\n",
    "visualize_data(cropped_data.copy(), 'Cropped Data', first_n_seconds=5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c76e8c320dc3a635",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can see the time axis has been shifted to start at +5 seconds. This should remove the noise as stated in the measurement concept. Next we can resample the data to a lower frequency as the original data could be sampled at a higher frequency than needed leading to an possible unnecessary high computational cost or not standardized data.",
   "id": "5b639183995295f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Resample\n",
    "\n",
    "Resampling the data to a lower frequency can help reduce the computational cost and standardize the data. For example, if the original data was sampled at 100 Hz, we can resample it to 50 Hz. This can be done using the `resample` method in pandas."
   ],
   "id": "55b63fee6aed897c"
  },
  {
   "cell_type": "code",
   "source": [
    "def visualize_resampling(original_data, resampled_data):\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    \n",
    "    plt.plot(original_data.index, original_data['x'], label='Original Data (X-axis)', alpha=0.5, linestyle='-', marker='o', markersize=4)\n",
    "    plt.plot(original_data.index, original_data['y'], label='Original Data (Y-axis)', alpha=0.5, linestyle='-', marker='o', markersize=4)\n",
    "    plt.plot(original_data.index, original_data['z'], label='Original Data (Z-axis)', alpha=0.5, linestyle='-', marker='o', markersize=4)\n",
    "    \n",
    "    plt.plot(resampled_data.index, resampled_data['x'], label='Resampled Data (X-axis)', linestyle='-', marker='x', markersize=7)\n",
    "    plt.plot(resampled_data.index, resampled_data['y'], label='Resampled Data (Y-axis)', linestyle='-', marker='x', markersize=7)\n",
    "    plt.plot(resampled_data.index, resampled_data['z'], label='Resampled Data (Z-axis)', linestyle='-', marker='x', markersize=7)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title('Comparison of Original and Resampled Data')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Values')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "numeric_columns = cropped_data.select_dtypes(include='number').columns\n",
    "cropped_data_numeric = cropped_data[numeric_columns]\n",
    "\n",
    "rate = f\"{int(1E6 / 50)}us\"\n",
    "resampled_data = cropped_data_numeric.resample(rate).mean()  # 100 Hz => 50 Hz == 1E6 / 50 us \n",
    "\n",
    "start_time = cropped_data_numeric.index[0]\n",
    "end_time = start_time + pd.Timedelta(seconds=1)\n",
    "cropped_data_subset = cropped_data_numeric[start_time:end_time]\n",
    "resampled_data_subset = resampled_data[start_time:end_time]\n",
    "\n",
    "visualize_resampling(cropped_data_subset, resampled_data_subset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "968da8b044df2093",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The resampled data is now at 50 Hz. We can see the data has been downsampled and the values have been averaged over the new time intervals. We can also notice that the lines do not overlap perfectly due to the averaging process. Overall, the characteristics of the data are preserved.",
   "id": "d09b5ff337159e0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Segmentation\n",
    "\n",
    "Segmentation involves dividing the data into smaller chunks. This is a necessary step for model training as the models are trained ona fixed context window. This can be done by specifying the segment size and overlap between segments. "
   ],
   "id": "207fffc4a2b55be5"
  },
  {
   "cell_type": "code",
   "source": [
    "segment_size = pd.Timedelta(seconds=5)\n",
    "overlap = pd.Timedelta(seconds=2)\n",
    "\n",
    "start_time = resampled_data.index.min()\n",
    "segments = []\n",
    "\n",
    "while start_time + segment_size <= resampled_data.index.max():\n",
    "    end_time = start_time + segment_size\n",
    "    segments.append(resampled_data[start_time:end_time])\n",
    "    start_time = end_time - overlap\n",
    "    \n",
    "print(f'Number of segments: {len(segments)}')\n",
    "visualize_data(segments[0], 'First Segment')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d056960bc6f928e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Overlap essentially means that the segments will have some overlapping data points. For example, if the segment size is 5 seconds and the overlap is 2 seconds, then the first segment will contain data points from 0 to 5 seconds, the second segment will contain data points from 3 to 8 seconds, and so on. This can be done by iterating over the data and creating segments of the specified size and overlap.",
   "id": "38102cf82c903b1b"
  },
  {
   "cell_type": "code",
   "source": [
    "def visualize_segment_overlap(segments, column='x'):    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, len(segments)))\n",
    "    \n",
    "    for i, segment in enumerate(segments):\n",
    "        plt.plot(segment.index, segment[column],\n",
    "                 label=f'Segment {i+1}', color=colors[i], alpha=0.7)\n",
    "    \n",
    "    plt.title(f'Visualization of Segment Overlap for Accelerometer {column.upper()}-axis')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value (with offset for visualization)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "visualize_segment_overlap(segments[:3])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9a89b1c071acf68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can see that the segments overlap by 2 seconds. This can server as a form of data augmentation as the model will see similar data points in different segments. This can help the model generalize better. Next we can extract features from the data.\n",
   "id": "69262ad79aabb225"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5. Feature Extraction\n",
    "\n",
    "Feature extraction involves extracting features from the data.\n",
    "\n",
    "## Frequency Domain\n",
    "\n",
    "The frequency domain can provide insights into the underlying patterns in the data. We can visualize the frequency domain using the Fast Fourier Transform (FFT)."
   ],
   "id": "36499ccbf2c1ea6b"
  },
  {
   "cell_type": "code",
   "source": [
    "def visualize_frequency_domain(data, sampling_rate, title='Frequency Domain'):\n",
    "    fft_result = np.fft.fft(data)\n",
    "    frequencies = np.fft.fftfreq(data.shape[0], 1/sampling_rate)\n",
    "    frequencies = np.fft.fftshift(frequencies)\n",
    "    fft_result = np.fft.fftshift(fft_result)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(frequencies, np.abs(fft_result))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.xlim(0, np.max(frequencies) / 2)\n",
    "    plt.show()\n",
    "\n",
    "visualize_frequency_domain(segments[0]['x'], 1E6 / 50, title='Frequency Domain for X-axis (First Segment)')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30b284cb53965506",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can see the frequency domain for the first segment of the X-axis data. We can see that the frequency domain provides insights into the underlying patterns in the data. We can also calculate the correlation between the different axes of the accelerometer data. There are some quite some high frequencies on the upper end of the spectrum. This can be due to the noise in the data.",
   "id": "ef820b2221703268"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Correlation\n",
    "\n",
    "We can calculate the correlation between the different axes of the accelerometer data. This can help us understand how the different axes are related to each other. We can visualize the correlation using a heatmap."
   ],
   "id": "f88ec877f380bed1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_correlation_heatmap(data, title='Correlation Heatmap'):\n",
    "    # rename xyz columns to accelerometer_x, accelerometer_y, accelerometer_z\n",
    "    data = data.copy().rename(columns={'x': 'accelerometer_x', 'y': 'accelerometer_y', 'z': 'accelerometer_z'})\n",
    "    \n",
    "    correlation_matrix = data.corr()\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', \n",
    "                fmt='.2f', cbar=True, square=True, annot_kws={'size': 10}, \n",
    "                mask=np.triu(correlation_matrix, k=1))\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "visualize_correlation_heatmap(segments[0].copy(), title='Correlation Heatmap for First Segment')"
   ],
   "id": "3bf3c6229826e683",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The correlation heatmap provides insights into expected and unexpected relationships among the sensor data axes. Strong correlations among orientation data axes are anticipated, as orientation components (quaternions and angles) typically have interdependencies due to their mathematical and physical relationships in representing 3D orientation. Additionally, the negative correlation between gravity axes, especially between gravity_x and gravity_z, is expected due to their orthogonal nature in a fixed reference frame, where an increase in one axis component often results in a decrease in another.\n",
    "\n",
    "On the other hand, weak correlations among gyroscope axes are surprising, as rotational movements usually involve interactions across multiple axes. This suggests that the gyroscope data might exhibit more complex dynamics or noise, leading to seemingly independent axis movements. These statements correctly reflect the relationships observed in the heatmap, providing a basis for further analysis and interpretation of the sensor data."
   ],
   "id": "e783f3db9c19266f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 6. Smoothing\n",
    "\n",
    "Smoothing involves applying filters to smooth the data. This can help remove noise and make the underlying patterns more visible. We can apply filters such as the Butterworth filter and the moving average filter.\n",
    "\n",
    "## Butterworth Filter\n",
    "\n",
    "The Butterworth filter is a type of signal processing filter designed to have a frequency response as flat as possible in the passband. It is often used in digital signal processing to reduce noise and improve the signal-to-noise ratio. We can apply the Butterworth filter to the accelerometer data."
   ],
   "id": "b84d574c41bf5c74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _calc_butterworth_filter(order, cutoff, sampling_rate):\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def apply_butterworth_filter(data, order, cutoff, sampling_rate):\n",
    "    b, a = _calc_butterworth_filter(order, cutoff, sampling_rate)\n",
    "    return signal.filtfilt(b, a, data)\n",
    "\n",
    "\n",
    "def visualize_butterworth_filter(data, filtered_data, title='Butterworth Filter'):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(data.index, data, label='Original Data', alpha=0.5)\n",
    "    plt.plot(data.index, filtered_data, label='Filtered Data', linestyle='--')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Values')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "butterworth_segment = segments.copy()[0]\n",
    "cutoff, order = 6, 4\n",
    "sampling_rate = 50\n",
    "for column in ['x', 'y', 'z']:\n",
    "    filtered_data = apply_butterworth_filter(butterworth_segment[column], order, cutoff, sampling_rate)\n",
    "    visualize_butterworth_filter(butterworth_segment[column], filtered_data, title=f'Butterworth Filter Application ({column}-axis, Order={order}, Cutoff={cutoff} Hz)')"
   ],
   "id": "a1cafff7d3ba283",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see, the Butterworth filter has been applied to the accelerometer data. The filter has removed some of the high-frequency noise from the data, making the underlying patterns more visible. We can also apply the moving average filter to the data. There is certainly a trade-off between the amount of noise removed and the amount of signal lost. The Butterworth filter is a low-pass filter, which means it removes high-frequency noise from the data. The cutoff frequency determines the frequency above which the filter starts to remove noise. The order of the filter determines the sharpness of the cutoff. A higher order filter will have a steeper cutoff but may introduce more distortion to the signal.",
   "id": "18325902ea35d55e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Moving Average\n",
    "\n",
    "The moving average filter is a simple low-pass filter that averages the data over a window of a specified size. This can help smooth the data and remove noise. We can apply the moving average filter to the accelerometer data."
   ],
   "id": "19020d5a7a99fd1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "moving_avg_segment = random.choice(segments).copy()\n",
    "\n",
    "def visualize_moving_average(data, filtered_data, title='Moving Average'):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(data.index, data, label='Original Data', alpha=0.5)\n",
    "    plt.plot(data.index, filtered_data, label='Filtered Data', linestyle='--')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Values')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def apply_moving_average(segment, window_size_s, sampling_rate, columns=['x', 'y', 'z']):\n",
    "    window_size = int(window_size_s * sampling_rate)\n",
    "    for column in columns:\n",
    "        original_data = segment[column].copy()\n",
    "        rolling_avg = segment[column].rolling(window=window_size).mean()\n",
    "        rolling_avg.iloc[:window_size] = rolling_avg.iloc[window_size]\n",
    "        segment.loc[:, column] = rolling_avg\n",
    "        visualize_moving_average(original_data, rolling_avg, \n",
    "                                 title=f'Moving Average Application ({column}-axis, Window Size={window_size_s} s)')\n",
    "\n",
    "apply_moving_average(moving_avg_segment.copy(), 0.2, sampling_rate)"
   ],
   "id": "a50ef3df2e7cf71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As well as the butterworth filter, the moving average filter has been applied to the accelerometer data. The filter has removed some of the high-frequency noise from the data, making the underlying patterns more visible. Overall the filtering seems rather aggressive and might remove key characteristics of the data. It is important to carefully choose the filter parameters to avoid removing important information from the data.\n",
    "\n",
    "In this case: The smaller the window size, the less noise is removed."
   ],
   "id": "d6aab2a2777f313f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "apply_moving_average(moving_avg_segment.copy(), .1, sampling_rate)",
   "id": "fa9fc40316a1faf9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
